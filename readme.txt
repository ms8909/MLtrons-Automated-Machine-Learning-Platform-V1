Next steps

1. Setup jupyter notebook
2. setup running of a datasets
3. setup test cases
4. write code for automatic report generation
5. run with infinite files on the cluster



Running locally vs running on the server.

Running on the server
1. git everything in the compiled code folder
2. make a virtual env on the server and set it up
3. write the code and test the code online for faster results








Setting up the deep learning model
1. since the deep learning model needs to get number of rows equal to the batch size on random, find out a wasy to get the number of rows
2. find a way to run keras on spark and in this way you might be able to find a way to train the model from spark dataframe.



